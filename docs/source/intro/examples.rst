.. _examples:

Examples
========

.. contents:: Table of Contents
   :depth: 1
   :local:
   :backlinks: none

.. raw:: html

   <hr>

Learning in an OpenAI Gym environment 
-------------------------------------

These examples perform the training of one agent in an OpenAI Gym environment (**one agent, one environment**)

.. image:: ../_static/imgs/example_gym.png
      :width: 100%
      :align: center
      :alt: OpenAI Gym environments

.. raw:: html

   <br>

The following components or practices are exemplified (highlighted):

    - Load and wrap an OpenAI Gym environment: **Pendulum (DDPG)**, **CartPole (CEM)**
    - Instantiate models using the model instantiation utility: **CartPole (DQN)**
    - Create a tabular model (:math:`\epsilon`-greedy policy): **Taxi (SARSA)**, **FrozenLake (Q-Learning)**
    - Load a checkpoint during evaluation: **Pendulum (DDPG)**, **CartPole (CEM)**, **CartPole (DQN)**, **Taxi (SARSA)**, **FrozenLake (Q-Learning)**

.. tabs::
            
    .. tab:: Pendulum (DDPG)

        .. tabs::
            
            .. group-tab:: Training

                :download:`gym_pendulum_ddpg.py <../examples/gym/gym_pendulum_ddpg.py>`

                .. literalinclude:: ../examples/gym/gym_pendulum_ddpg.py
                    :language: python
                    :emphasize-lines: 1, 13, 50-56

            .. group-tab:: Evaluation
                
                :download:`gym_pendulum_ddpg_eval.py <../examples/gym/gym_pendulum_ddpg_eval.py>`

                **Note:** It is necessary to adjust the checkpoint path according to the directories generated by the new experiments

                **Note:** Warnings such as :literal:`[skrl:WARNING] Cannot load the <module> module. The agent doesn't have such an instance` can be ignored without problems. The reason for this is that during the evaluation, not all components such as optimizers or other models apart from the policy are defined

                .. literalinclude:: ../examples/gym/gym_pendulum_ddpg_eval.py
                    :language: python
                    :emphasize-lines: 67

    .. tab:: CartPole (CEM)

        .. tabs::

            .. group-tab:: Training

                :download:`gym_cartpole_cem.py <../examples/gym/gym_cartpole_cem.py>`

                .. literalinclude:: ../examples/gym/gym_cartpole_cem.py
                    :language: python
                    :emphasize-lines: 1, 11, 33-39

            .. group-tab:: Evaluation

                :download:`gym_cartpole_cem_eval.py <../examples/gym/gym_cartpole_cem_eval.py>`

                **Note:** It is necessary to adjust the checkpoint path according to the directories generated by the new experiments

                **Note:** Warnings such as :literal:`[skrl:WARNING] Cannot load the <module> module. The agent doesn't have such an instance` can be ignored without problems. The reason for this is that during the evaluation, not all components such as optimizers or other models apart from the policy are defined

                .. literalinclude:: ../examples/gym/gym_cartpole_cem_eval.py
                    :language: python
                    :emphasize-lines: 68

    .. tab:: CartPole (DQN)

        .. tabs::
            
            .. group-tab:: Training
                
                :download:`gym_cartpole_dqn.py <../examples/gym/gym_cartpole_dqn.py>`

                .. literalinclude:: ../examples/gym/gym_cartpole_dqn.py
                    :language: python
                    :emphasize-lines: 4, 31-51
        
            .. group-tab:: Evaluation
                
                :download:`gym_cartpole_dqn_eval.py <../examples/gym/gym_cartpole_dqn_eval.py>`
                
                **Note:** It is necessary to adjust the checkpoint path according to the directories generated by the new experiments

                **Note:** Warnings such as :literal:`[skrl:WARNING] Cannot load the <module> module. The agent doesn't have such an instance` can be ignored without problems. The reason for this is that during the evaluation, not all components such as optimizers or other models apart from the policy are defined

                .. literalinclude:: ../examples/gym/gym_cartpole_dqn_eval.py
                    :language: python
                    :emphasize-lines: 56
    
    .. tab:: Taxi (SARSA)

        .. tabs::
            
            .. group-tab:: Training
                
                :download:`gym_taxi_sarsa.py <../examples/gym/gym_taxi_sarsa.py>`

                .. literalinclude:: ../examples/gym/gym_taxi_sarsa.py
                    :language: python
                    :emphasize-lines: 6, 13-30
        
            .. group-tab:: Evaluation
                
                :download:`gym_taxi_sarsa_eval.py <../examples/gym/gym_taxi_sarsa_eval.py>`
                
                **Note:** It is necessary to adjust the checkpoint path according to the directories generated by the new experiments

                **Note:** Warnings such as :literal:`[skrl:WARNING] Cannot load the <module> module. The agent doesn't have such an instance` can be ignored without problems. The reason for this is that during the evaluation, not all components such as optimizers or other models apart from the policy are defined

                .. literalinclude:: ../examples/gym/gym_taxi_sarsa_eval.py
                    :language: python
                    :emphasize-lines: 70
    
    .. tab:: FrozenLake (Q-learning)

        .. tabs::
            
            .. group-tab:: Training
                
                :download:`gym_frozen_lake_q_learning.py <../examples/gym/gym_frozen_lake_q_learning.py>`

                .. literalinclude:: ../examples/gym/gym_frozen_lake_q_learning.py
                    :language: python
                    :emphasize-lines: 6, 13-30
        
            .. group-tab:: Evaluation
                
                :download:`gym_frozen_lake_q_learning_eval.py <../examples/gym/gym_frozen_lake_q_learning_eval.py>`
                
                **Note:** It is necessary to adjust the checkpoint path according to the directories generated by the new experiments

                **Note:** Warnings such as :literal:`[skrl:WARNING] Cannot load the <module> module. The agent doesn't have such an instance` can be ignored without problems. The reason for this is that during the evaluation, not all components such as optimizers or other models apart from the policy are defined

                .. literalinclude:: ../examples/gym/gym_frozen_lake_q_learning_eval.py
                    :language: python
                    :emphasize-lines: 70

.. raw:: html

   <hr>

Learning in an OpenAI Gym vectorized environment
------------------------------------------------

These examples perform the training of one agent in an OpenAI Gym vectorized environment (**one agent, multiple independent copies of the same environment in parallel**)

The following components or practices are exemplified (highlighted):

    - Load and wrap an OpenAI Gym vectorized environment: **Pendulum (DDPG)**, **CartPole (DQN)**, **Taxi (SARSA)**, **FrozenLake (Q-Learning)**

.. tabs::
            
    .. tab:: Pendulum (DDPG)

        .. tabs::
            
            .. group-tab:: Training

                :download:`gym_vector_pendulum_ddpg.py <../examples/gym/gym_vector_pendulum_ddpg.py>`

                .. literalinclude:: ../examples/gym/gym_vector_pendulum_ddpg.py
                    :language: python
                    :emphasize-lines: 1, 13, 50-56

    .. tab:: CartPole (DQN)

        .. tabs::
            
            .. group-tab:: Training
                
                :download:`gym_vector_cartpole_dqn.py <../examples/gym/gym_vector_cartpole_dqn.py>`

                .. literalinclude:: ../examples/gym/gym_vector_cartpole_dqn.py
                    :language: python
                    :emphasize-lines: 1, 8, 13-19
    
    .. tab:: Taxi (SARSA)

        .. tabs::
            
            .. group-tab:: Training
                
                :download:`gym_vector_taxi_sarsa.py <../examples/gym/gym_vector_taxi_sarsa.py>`

                .. literalinclude:: ../examples/gym/gym_vector_taxi_sarsa.py
                    :language: python
                    :emphasize-lines: 1, 9, 35-41
    
    .. tab:: FrozenLake (Q-learning)

        .. tabs::
            
            .. group-tab:: Training
                
                :download:`gym_vector_frozen_lake_q_learning.py <../examples/gym/gym_vector_frozen_lake_q_learning.py>`

                .. literalinclude:: ../examples/gym/gym_vector_frozen_lake_q_learning.py
                    :language: python
                    :emphasize-lines: 1, 9, 35-41

.. raw:: html

   <hr>

Learning in a DeepMind environment
----------------------------------

These examples perform the training of one agent in an DeepMind environment (**one agent, one environment**)

.. image:: ../_static/imgs/example_deepmind.png
      :width: 100%
      :align: center
      :alt: DeepMind environments

.. raw:: html

   <br>

The following components or practices are exemplified (highlighted):

    - Load and wrap a DeepMind environment: **cartpole (DDPG)**
    - Map the observation/state space (flat tensor) to the original environment space to be used by the model: **reach_site_vision (SAC)**

.. tabs::
            
    .. tab:: suite:cartpole (DDPG)

        .. tabs::
            
            .. group-tab:: Training

                :download:`dm_suite_cartpole_swingup_ddpg.py <../examples/deepmind/dm_suite_cartpole_swingup_ddpg.py>`

                .. literalinclude:: ../examples/deepmind/dm_suite_cartpole_swingup_ddpg.py
                    :language: python
                    :emphasize-lines: 1, 13, 50-51
    
    .. tab:: manipulation:reach_site_vision (SAC)

        .. tabs::
            
            .. group-tab:: Training

                :download:`dm_manipulation_stack_sac.py <../examples/deepmind/dm_manipulation_stack_sac.py>`

                .. literalinclude:: ../examples/deepmind/dm_manipulation_stack_sac.py
                    :language: python
                    :emphasize-lines: 67, 80, 83-84, 112, 115, 118-119

.. raw:: html

   <hr>

Learning in an Isaac Gym environment
------------------------------------

These examples perform the training of an agent in the `Isaac Gym environments <https://github.com/NVIDIA-Omniverse/IsaacGymEnvs>`_ (**one agent, multiple environments**)

.. image:: ../_static/imgs/example_isaacgym.png
      :width: 100%
      :align: center
      :alt: Isaac Gym environments

.. raw:: html

   <br>

The following components or practices are exemplified (highlighted):

    - Load an Isaac Gym environment (easy-to-use API from NVIDIA): **AllegroHand**, **Ingenuity**
    - Load and wrap an Isaac Gym environment: **Ant**, **Anymal**
    - Set an input preprocessor: **AnymalTerrain**, **BallBalance**
    - Set a random seed for reproducibility: **Cartpole**
    - Set a learning rate scheduler: **FrankaCabinet**, **Humanoid**
    - Define a reward shaping function: **Quadcopter**, **ShadowHand**, **Trifinger**
    - Access to environment-specific properties and methods: **Humanoid (AMP)**
    - Load a checkpoint during evaluation: **Cartpole**

The PPO agent configuration is mapped, as far as possible, from the rl_games' A2C-PPO `configuration for Isaac Gym preview environments <https://github.com/NVIDIA-Omniverse/IsaacGymEnvs/tree/main/isaacgymenvs/cfg/train>`_. Shared models or separated models are used depending on the value of the :literal:`network.separate` variable. The following list shows the mapping between the two configurations:

.. code-block:: bash

    # memory
    memory_size = horizon_length

    # agent
    rollouts = horizon_length
    learning_epochs = mini_epochs
    mini_batches = horizon_length * num_actors / minibatch_size
    discount_factor = gamma
    lambda = tau
    learning_rate = learning_rate
    learning_rate_scheduler = skrl.resources.schedulers.torch.KLAdaptiveRL
    learning_rate_scheduler_kwargs = {"kl_threshold": kl_threshold}
    random_timesteps = 0
    learning_starts = 0
    grad_norm_clip = grad_norm
    ratio_clip = e_clip
    value_clip = e_clip
    clip_predicted_values = clip_value
    entropy_loss_scale = entropy_coef
    value_loss_scale = 0.5 * critic_coef
    kl_threshold = 0
    rewards_shaper = lambda rewards, timestep, timesteps: rewards * scale_value

    # trainer
    timesteps = horizon_length * max_epochs

**Benchmark results** for Isaac Gym are listed in `Benchmark results #32 <https://github.com/Toni-SM/skrl/discussions/32>`_.

.. note::

    Isaac Gym environments implement a functionality to get their configuration from the command line. Because of this feature, setting the :literal:`headless` option from the trainer configuration will not work. In this case, it is necessary to invoke the scripts as follows: :literal:`python script.py headless=True` for Isaac Gym environments (preview 3 and preview 4) or :literal:`python script.py --headless` for Isaac Gym environments (preview 2)

.. tabs::
            
    .. tab:: Isaac Gym environments (training)

        .. tabs::
            
            .. tab:: AllegroHand
                
                :download:`ppo_allegro_hand.py <../examples/isaacgym/ppo_allegro_hand.py>`

                .. literalinclude:: ../examples/isaacgym/ppo_allegro_hand.py
                    :language: python
                    :emphasize-lines: 2, 19, 56-62

            .. tab:: Ant
                
                :download:`ppo_ant.py <../examples/isaacgym/ppo_ant.py>`

                .. literalinclude:: ../examples/isaacgym/ppo_ant.py
                    :language: python
                    :emphasize-lines: 13-14, 56-57

            .. tab:: Anymal
                
                :download:`ppo_anymal.py <../examples/isaacgym/ppo_anymal.py>`

                .. literalinclude:: ../examples/isaacgym/ppo_anymal.py
                    :language: python
                    :emphasize-lines: 13-14, 56-57

            .. tab:: AnymalTerrain
                
                :download:`ppo_anymal_terrain.py <../examples/isaacgym/ppo_anymal_terrain.py>`

                .. literalinclude:: ../examples/isaacgym/ppo_anymal_terrain.py
                    :language: python
                    :emphasize-lines: 11, 101-104

            .. tab:: BallBalance
                
                :download:`ppo_ball_balance.py <../examples/isaacgym/ppo_ball_balance.py>`

                .. literalinclude:: ../examples/isaacgym/ppo_ball_balance.py
                    :language: python
                    :emphasize-lines: 11, 96-99

            .. tab:: Cartpole
                
                :download:`ppo_cartpole.py <../examples/isaacgym/ppo_cartpole.py>`

                .. literalinclude:: ../examples/isaacgym/ppo_cartpole.py
                    :language: python
                    :emphasize-lines: 15, 19

            .. tab:: Cartpole (TRPO)
                
                :download:`trpo_cartpole.py <../examples/isaacgym/trpo_cartpole.py>`

                .. literalinclude:: ../examples/isaacgym/trpo_cartpole.py
                    :language: python
                    :emphasize-lines: 14, 18

            .. tab:: FrankaCabinet
                
                :download:`ppo_franka_cabinet.py <../examples/isaacgym/ppo_franka_cabinet.py>`

                .. literalinclude:: ../examples/isaacgym/ppo_franka_cabinet.py
                    :language: python
                    :emphasize-lines: 10, 84-85

            .. tab:: Humanoid
                
                :download:`ppo_humanoid.py <../examples/isaacgym/ppo_humanoid.py>`

                .. literalinclude:: ../examples/isaacgym/ppo_humanoid.py
                    :language: python
                    :emphasize-lines: 10, 84-85

            .. tab:: Humanoid (AMP)
                
                :download:`amp_humanoid.py <../examples/isaacgym/amp_humanoid.py>`

                .. literalinclude:: ../examples/isaacgym/amp_humanoid.py
                    :language: python
                    :emphasize-lines: 89, 124, 135, 138-139

            .. tab:: Ingenuity
                
                :download:`ppo_ingenuity.py <../examples/isaacgym/ppo_ingenuity.py>`

                .. literalinclude:: ../examples/isaacgym/ppo_ingenuity.py
                    :language: python
                    :emphasize-lines: 2, 19, 56-62

            .. tab:: Quadcopter
                
                :download:`ppo_quadcopter.py <../examples/isaacgym/ppo_quadcopter.py>`

                .. literalinclude:: ../examples/isaacgym/ppo_quadcopter.py
                    :language: python
                    :emphasize-lines: 95

            .. tab:: ShadowHand
                
                :download:`ppo_shadow_hand.py <../examples/isaacgym/ppo_shadow_hand.py>`

                .. literalinclude:: ../examples/isaacgym/ppo_shadow_hand.py
                    :language: python
                    :emphasize-lines: 97

            .. tab:: Trifinger
                
                :download:`ppo_trifinger.py <../examples/isaacgym/ppo_trifinger.py>`

                .. literalinclude:: ../examples/isaacgym/ppo_trifinger.py
                    :language: python
                    :emphasize-lines: 95

    .. tab:: Isaac Gym environments (evaluation)

        .. tabs::
            
            .. tab:: Cartpole
                
                :download:`ppo_cartpole_eval.py <../examples/isaacgym/ppo_cartpole_eval.py>`
                
                **Note:** It is necessary to adjust the checkpoint path according to the directories generated by the new experiments

                **Note:** Warnings such as :literal:`[skrl:WARNING] Cannot load the <module> module. The agent doesn't have such an instance` can be ignored without problems. The reason for this is that during the evaluation, not all components such as optimizers or other models apart from the policy are defined

                .. literalinclude:: ../examples/isaacgym/ppo_cartpole_eval.py
                    :language: python
                    :emphasize-lines: 65

.. raw:: html

   <hr>

Learning by scopes in an Isaac Gym environment
----------------------------------------------

These examples perform the training of 3 agents by scopes in Isaac Gym's Cartpole environment in the same run (**multiple agents and environments**)

.. image:: ../_static/imgs/example_parallel.jpg
      :width: 100%
      :align: center
      :alt: Simultaneous training

.. raw:: html

   <br>

Two versions are presented:

    - Simultaneous (sequential) training of agents **sharing the same memory** and whose scopes are automatically selected as equally as possible
    - Simultaneous (sequential and parallel) training and evaluation of agents **with local memory** (no memory sharing) and whose scopes are manually specified and differ from each other

The following components or practices are exemplified (highlighted):

    - Create a shared memory: **Shared memory**
    - Learning by scopes (automatically defined): **Shared memory**
    - Create non-shared memories: **No shared memory**
    - Learning by scopes (manually defined): **No shared memory**
    - Load a checkpoint during evaluation: **Shared memory**, **No shared memory**

.. note::

    Isaac Gym environments implement a functionality to get their configuration from the command line. Because of this feature, setting the :literal:`headless` option from the trainer configuration will not work. In this case, it is necessary to invoke the scripts as follows: :literal:`python script.py headless=True` for Isaac Gym environments (preview 3 and preview 4) or :literal:`python script.py --headless` for Isaac Gym environments (preview 2)
    
.. tabs::
            
    .. tab:: Shared memory

        .. tabs::
            
            .. tab:: Sequential training
                
                :download:`isaacgym_sequential_shared_memory.py <../examples/isaacgym/isaacgym_sequential_shared_memory.py>`

                .. literalinclude:: ../examples/isaacgym/isaacgym_sequential_shared_memory.py
                    :language: python
                    :emphasize-lines: 75, 149, 156, 163, 174-175

            .. tab:: Sequential evaluation
                
                :download:`isaacgym_sequential_shared_memory_eval.py <../examples/isaacgym/isaacgym_sequential_shared_memory_eval.py>`
                
                **Note:** It is necessary to adjust the checkpoint path according to the directories generated by the new experiments

                **Note:** Warnings such as :literal:`[skrl:WARNING] Cannot load the <module> module. The agent doesn't have such an instance` can be ignored without problems. The reason for this is that during the evaluation, not all components such as optimizers or other models apart from the policy are defined

                .. literalinclude:: ../examples/isaacgym/isaacgym_sequential_shared_memory_eval.py
                    :language: python
                    :emphasize-lines: 113-115, 126

    .. tab:: No shared memory

        .. tabs::
            
            .. tab:: Sequential training
                
                :download:`isaacgym_sequential_no_shared_memory.py <../examples/isaacgym/isaacgym_sequential_no_shared_memory.py>`

                .. literalinclude:: ../examples/isaacgym/isaacgym_sequential_no_shared_memory.py
                    :language: python
                    :emphasize-lines: 75-77, 151, 158, 165, 176-177

            .. tab:: Parallel training
                
                :download:`isaacgym_parallel_no_shared_memory.py <../examples/isaacgym/isaacgym_parallel_no_shared_memory.py>`

                .. literalinclude:: ../examples/isaacgym/isaacgym_parallel_no_shared_memory.py
                    :language: python
                    :emphasize-lines: 13, 67, 176-179

            .. tab:: Sequential eval...
                
                :download:`isaacgym_sequential_no_shared_memory_eval.py <../examples/isaacgym/isaacgym_sequential_no_shared_memory_eval.py>`
                
                **Note:** It is necessary to adjust the checkpoint path according to the directories generated by the new experiments

                **Note:** Warnings such as :literal:`[skrl:WARNING] Cannot load the <module> module. The agent doesn't have such an instance` can be ignored without problems. The reason for this is that during the evaluation, not all components such as optimizers or other models apart from the policy are defined

                .. literalinclude:: ../examples/isaacgym/isaacgym_sequential_no_shared_memory_eval.py
                    :language: python
                    :emphasize-lines: 113-115, 126

            .. tab:: Parallel eval...
                
                :download:`isaacgym_parallel_no_shared_memory_eval.py <../examples/isaacgym/isaacgym_parallel_no_shared_memory_eval.py>`
                
                **Note:** It is necessary to adjust the checkpoint path according to the directories generated by the new experiments

                **Note:** Warnings such as :literal:`[skrl:WARNING] Cannot load the <module> module. The agent doesn't have such an instance` can be ignored without problems. The reason for this is that during the evaluation, not all components such as optimizers or other models apart from the policy are defined

                .. literalinclude:: ../examples/isaacgym/isaacgym_parallel_no_shared_memory_eval.py
                    :language: python
                    :emphasize-lines: 115-117, 128

.. raw:: html

   <hr>

Learning in an Omniverse Isaac Gym environment
----------------------------------------------

These examples perform the training of an agent in the `Omniverse Isaac Gym environments <https://github.com/NVIDIA-Omniverse/OmniIsaacGymEnvs>`_ (**one agent, multiple environments**)

.. image:: ../_static/imgs/example_omniverse_isaacgym.png
      :width: 100%
      :align: center
      :alt: Isaac Gym environments

.. raw:: html

   <br>

The following components or practices are exemplified (highlighted):

    - Load and wrap an Omniverse Isaac Gym environment: **AllegroHand**, **Ant**, **Anymal**
    - Load and wrap an Omniverse Isaac Gym multi-threaded environment: **Ant (multi-threaded)**, **Cartpole (multi-threaded)**
    - Set an input preprocessor: **AnymalTerrain**, **BallBalance**
    - Set a random seed for reproducibility: **Cartpole**, **Crazyflie**
    - Set a learning rate scheduler: **FrankaCabinet**, **Humanoid**
    - Define a reward shaping function: **Ingenuity**, **Quadcopter**, **ShadowHand**

The PPO agent configuration is mapped, as far as possible, from the rl_games' A2C-PPO `configuration for Omniverse Isaac Gym environments <https://github.com/NVIDIA-Omniverse/OmniIsaacGymEnvs/tree/main/omniisaacgymenvs/cfg/train>`_. Shared models or separated models are used depending on the value of the :literal:`network.separate` variable. The following list shows the mapping between the two configurations:configurations

.. code-block:: bash

    # memory
    memory_size = horizon_length

    # agent
    rollouts = horizon_length
    learning_epochs = mini_epochs
    mini_batches = horizon_length * num_actors / minibatch_size
    discount_factor = gamma
    lambda = tau
    learning_rate = learning_rate
    learning_rate_scheduler = skrl.resources.schedulers.torch.KLAdaptiveRL
    learning_rate_scheduler_kwargs = {"kl_threshold": kl_threshold}
    random_timesteps = 0
    learning_starts = 0
    grad_norm_clip = grad_norm
    ratio_clip = e_clip
    value_clip = e_clip
    clip_predicted_values = clip_value
    entropy_loss_scale = entropy_coef
    value_loss_scale = 0.5 * critic_coef
    kl_threshold = 0
    rewards_shaper = lambda rewards, timestep, timesteps: rewards * scale_value

    # trainer
    timesteps = horizon_length * max_epochs

**Benchmark results** for Omniverse Isaac Gym are listed in `Benchmark results #32 <https://github.com/Toni-SM/skrl/discussions/32>`_.

.. note::

    Omniverse Isaac Gym environments implement a functionality to get their configuration from the command line. Because of this feature, setting the :literal:`headless` option from the trainer configuration will not work. In this case, it is necessary to invoke the scripts as follows: :literal:`python script.py headless=True`

.. tabs::

    .. tab:: Omniverse Isaac Gym (training)

        .. tabs::

            .. tab:: AllegroHand
                
                :download:`ppo_allegro_hand.py <../examples/omniisaacgym/ppo_allegro_hand.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_allegro_hand.py
                    :language: python
                    :emphasize-lines: 11-12, 54-55
            
            .. tab:: Ant
                
                :download:`ppo_ant.py <../examples/omniisaacgym/ppo_ant.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_ant.py
                    :language: python
                    :emphasize-lines: 11-12, 54-55

            .. tab:: Ant (multi-threaded)
                
                :download:`ppo_ant_mt.py <../examples/omniisaacgym/ppo_ant_mt.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_ant_mt.py
                    :language: python
                    :emphasize-lines: 1, 13-14, 56-57, 117, 121
            
            .. tab:: Anymal
                
                :download:`ppo_anymal.py <../examples/omniisaacgym/ppo_anymal.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_anymal.py
                    :language: python
                    :emphasize-lines: 11-12, 54-55

            .. tab:: AnymalTerrain
                
                :download:`ppo_anymal_terrain.py <../examples/omniisaacgym/ppo_anymal_terrain.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_anymal_terrain.py
                    :language: python
                    :emphasize-lines: 9, 99-102

            .. tab:: BallBalance

                :download:`ppo_ball_balance.py <../examples/omniisaacgym/ppo_ball_balance.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_ball_balance.py
                    :language: python
                    :emphasize-lines: 9, 94-97

            .. tab:: Cartpole
                
                :download:`ppo_cartpole.py <../examples/omniisaacgym/ppo_cartpole.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_cartpole.py
                    :language: python
                    :emphasize-lines: 13, 17

            .. tab:: Cartpole (multi-threaded)
                
                :download:`ppo_cartpole_mt.py <../examples/omniisaacgym/ppo_cartpole_mt.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_cartpole_mt.py
                    :language: python
                    :emphasize-lines: 1, 13-14, 54-55, 115, 119

            .. tab:: Crazyflie
                
                :download:`ppo_crazy_flie.py <../examples/omniisaacgym/ppo_crazy_flie.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_crazy_flie.py
                    :language: python
                    :emphasize-lines: 13, 17

            .. tab:: FrankaCabinet
                
                :download:`ppo_franka_cabinet.py <../examples/omniisaacgym/ppo_franka_cabinet.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_franka_cabinet.py
                    :language: python
                    :emphasize-lines: 8, 82-83

            .. tab:: Humanoid
                
                :download:`ppo_humanoid.py <../examples/omniisaacgym/ppo_humanoid.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_humanoid.py
                    :language: python
                    :emphasize-lines: 8, 82-83
                    
            .. tab:: Ingenuity
                
                :download:`ppo_ingenuity.py <../examples/omniisaacgym/ppo_ingenuity.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_ingenuity.py
                    :language: python
                    :emphasize-lines: 93

            .. tab:: Quadcopter
                
                :download:`ppo_quadcopter.py <../examples/omniisaacgym/ppo_quadcopter.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_quadcopter.py
                    :language: python
                    :emphasize-lines: 93

            .. tab:: ShadowHand
                
                :download:`ppo_shadow_hand.py <../examples/omniisaacgym/ppo_shadow_hand.py>`

                .. literalinclude:: ../examples/omniisaacgym/ppo_shadow_hand.py
                    :language: python
                    :emphasize-lines: 95

.. raw:: html

   <hr>

Learning in an Omniverse Isaac Sim environment
----------------------------------------------

These examples show how to train an agent in an Omniverse Isaac Sim environment that is implemented using the OpenAI Gym interface (**one agent, one environment**)

.. tabs::

    .. tab:: Isaac Sim 2022.1.X (Cartpole)

        This example performs the training of an agent in the Isaac Sim's Cartpole environment described in the `Creating New RL Environment <https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_gym_new_rl_example.html>`_ tutorial

        Use the steps described below to setup and launch the experiment after follow the tutorial

        .. code-block:: bash
            
            # download the sample code from GitHub in the directory containing the cartpole_task.py script
            wget https://raw.githubusercontent.com/Toni-SM/skrl/main/docs/source/examples/isaacsim/cartpole_example_skrl.py

            # run the experiment
            PYTHON_PATH cartpole_example_skrl.py

        .. raw:: html

            <br>

        :download:`cartpole_example_skrl.py <../examples/isaacsim/cartpole_example_skrl.py>`

        .. literalinclude:: ../examples/isaacsim/cartpole_example_skrl.py
            :language: python

    .. tab:: Isaac Sim 2021.2.1 (JetBot)
   
        This example performs the training of an agent in the Isaac Sim's JetBot environment. The following components or practices are exemplified (highlighted):

        - Define and instantiate Convolutional Neural Networks (CNN) to learn from 128 X 128 RGB images

        Use the steps described below (for a local workstation or a remote container) to setup and launch the experiment

        .. tabs::

            .. tab:: Local workstation (setup)
                
                .. code-block:: bash

                    # create a working directory and change to it
                    mkdir ~/.local/share/ov/pkg/isaac_sim-2021.2.1/standalone_examples/api/omni.isaac.jetbot/skrl_example 
                    cd ~/.local/share/ov/pkg/isaac_sim-2021.2.1/standalone_examples/api/omni.isaac.jetbot/skrl_example 

                    # install the skrl library in editable mode from the working directory
                    ~/.local/share/ov/pkg/isaac_sim-2021.2.1/python.sh -m pip install -e git+https://github.com/Toni-SM/skrl.git#egg=skrl

                    # download the sample code from GitHub
                    wget https://raw.githubusercontent.com/Toni-SM/skrl/main/docs/source/examples/isaacsim/isaacsim_jetbot_ppo.py

                    # copy the Isaac Sim sample environment (JetBotEnv) to the working directory
                    cp ../stable_baselines_example/env.py .

                    # run the experiment
                    ~/.local/share/ov/pkg/isaac_sim-2021.2.1/python.sh isaacsim_jetbot_ppo.py

            .. tab:: Remote container (setup)

                .. code-block:: bash

                    # create a working directory and change to it
                    mkdir /isaac-sim/standalone_examples/api/omni.isaac.jetbot/skrl_example 
                    cd /isaac-sim/standalone_examples/api/omni.isaac.jetbot/skrl_example

                    # install the skrl library in editable mode from the working directory
                    /isaac-sim/kit/python/bin/python3 -m pip install -e git+https://github.com/Toni-SM/skrl.git#egg=skrl

                    # download the sample code from GitHub
                    wget https://raw.githubusercontent.com/Toni-SM/skrl/main/docs/source/examples/isaacsim/isaacsim_jetbot_ppo.py

                    # copy the Isaac Sim sample environment (JetBotEnv) to the working directory
                    cp ../stable_baselines_example/env.py .

                    # run the experiment
                    /isaac-sim/python.sh isaacsim_jetbot_ppo.py
                
        .. raw:: html

            <br>

        :download:`isaacsim_jetbot_ppo.py <../examples/isaacsim/isaacsim_jetbot_ppo.py>`

        .. literalinclude:: ../examples/isaacsim/isaacsim_jetbot_ppo.py
            :language: python
            :emphasize-lines: 24-39, 45, 53-68, 73

Real-world examples
-------------------

These examples show basic real-world use cases to guide and support advanced RL implementations

.. tabs::

    .. tab:: Franka Emika Panda

        **3D reaching task (Franka's gripper must reach a certain target point in space)**. The training was done in Omniverse Isaac Gym. The real robot control is performed through the Python API of a modified version of frankx (see `frankx's pull request #44 <https://github.com/pantor/frankx/pull/44>`_), a high-level motion library around libfranka. Training and evaluation is performed for both Cartesian and joint control space

        .. raw:: html

            <hr>
        
        **Implementation** (see details in the table below):

        * The observation space is composed of the episode's normalized progress, the robot joints' normalized positions (:math:`q`) in the interval -1 to 1, the robot joints' velocities (:math:`\dot{q}`) affected by a random uniform scale for generalization, and the target's position in space (:math:`target_{_{XYZ}}`) with respect to the robot's base
        
        * The action space, bounded in the range -1 to 1, consists of the following. For the joint control it's robot joints' position scaled change. For the Cartesian control it's the end-effector's position (:math:`ee_{_{XYZ}}`) scaled change. The end-effector position frame corresponds to the point where the left finger connects to the gripper base in simulation, whereas in the real world it corresponds to the end of the fingers. The gripper fingers remain closed all the time in both cases
        
        * The instantaneous reward is the negative value of the Euclidean distance (:math:`\text{d}`) between the robot end-effector and the target point position. The episode terminates when this distance is less than 0.035 meters in simulation (0.075 meters in real-world) or when the defined maximum timestep is reached

        * The target position lies within a rectangular cuboid of dimensions 0.5 x 0.5 x 0.2 meters centered at 0.5, 0.0, 0.2 meters with respect to the robot's base. The robot joints' positions are drawn from an initial configuration [0º, -45º, 0º, -135º, 0º, 90º, 45º] modified with uniform random values between -7º and 7º approximately

        .. list-table::
            :header-rows: 1

            * - Variable
              - Formula / value
              - Size
            * - Observation space
              - :math:`\dfrac{t}{t_{max}},\; 2 \dfrac{q - q_{min}}{q_{max} - q_{min}} - 1,\; 0.1\,\dot{q}\,U(0.5,1.5),\; target_{_{XYZ}}` 
              - 18
            * - Action space (joint)
              - :math:`\dfrac{2.5}{120} \, \Delta q`
              - 7
            * - Action space (Cartesian)
              - :math:`\dfrac{1}{100} \, \Delta ee_{_{XYZ}}`
              - 3
            * - Reward
              - :math:`-\text{d}(ee_{_{XYZ}},\; target_{_{XYZ}})`
              - 
            * - Episode termination
              - :math:`\text{d}(ee_{_{XYZ}},\; target_{_{XYZ}}) \le 0.035 \quad` or :math:`\quad t \ge t_{max} - 1`   
              - 
            * - Maximum timesteps (:math:`t_{max}`)
              - 100
              - 

        .. raw:: html

            <hr>

        **Workflows**

        .. tabs::

            .. tab:: Real-world

                .. warning::

                    Make sure you have the e-stop on hand in case something goes wrong in the run. **Control via RL can be dangerous and unsafe for both the operator and the robot**

                .. raw:: html

                    <video width="100%" controls autoplay>
                        <source src="https://user-images.githubusercontent.com/22400377/190899202-6b80c48d-fc49-48e9-b277-24814d0adab1.mp4" type="video/mp4">
                    </video>
                    <strong>Target position entered via the command prompt or generated randomly</strong>
                    <br><br>
                    <video width="100%" controls autoplay>
                        <source src="https://user-images.githubusercontent.com/22400377/190899205-752f654e-9310-4696-a6b2-bfa57d5325f2.mp4" type="video/mp4">
                    </video>
                    <strong>Target position in X and Y obtained with a USB-camera (position in Z fixed at 0.2 m)</strong>

                |

                **Prerequisites:**

                A physical Franka robot with `Franka Control Interface (FCI) <https://frankaemika.github.io/docs/index.html>`_ is required. Additionally, the frankx library must be available in the python environment (see `frankx's pull request #44 <https://github.com/pantor/frankx/pull/44>`_ for the RL-compatible version installation)

                **Files**

                * Environment: :download:`reaching_franka_real_env.py <../examples/real_world/franka_emika_panda/reaching_franka_real_env.py>`
                * Evaluation script: :download:`reaching_franka_real_skrl_eval.py <../examples/real_world/franka_emika_panda/reaching_franka_real_skrl_eval.py>`
                * Checkpoints (:literal:`agent_joint.pt`, :literal:`agent_cartesian.pt`): :download:`trained_checkpoints.zip <https://github.com/Toni-SM/skrl/files/9595293/trained_checkpoints.zip>`

                **Evaluation:**

                .. code-block:: bash

                    python3 reaching_franka_real_skrl_eval.py

                **Main environment configuration:**

                .. note::

                    In the joint control space the final control of the robot is performed through the Cartesian pose (forward kinematics from specified values for the joints)

                The control space (Cartesian or joint), the robot motion type (waypoint or impedance) and the target position acquisition (command prompt / automatically generated or USB-camera) can be specified in the environment class constructor (from :literal:`reaching_franka_real_skrl_eval.py`) as follow:

                .. code-block:: python

                    control_space = "joint"   # joint or cartesian
                    motion_type = "waypoint"  # waypoint or impedance
                    camera_tracking = False   # True for USB-camera tracking

            .. tab:: Simulation (Omniverse Isaac Gym)

                .. raw:: html

                    <video width="100%" controls autoplay>
                        <source src="https://user-images.githubusercontent.com/22400377/190926792-6e788eaf-1600-4b13-b8c8-e0e0a09e4827.mp4" type="video/mp4">
                    </video>

                .. raw:: html

                    <img width="100%" src="https://user-images.githubusercontent.com/22400377/190921341-6feb255a-04d4-4e51-bc7a-f939116dd02d.png">

                |

                **Prerequisites:**

                All installation steps described in Omniverse Isaac Gym's `Overview & Getting Started <https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_gym_isaac_gym.html>`_ section must be fulfilled (especially the subsection 1.3. Installing Examples Repository)

                **Files** (the implementation is self-contained so no specific location is required):

                * Environment: :download:`reaching_franka_omniverse_isaacgym_env.py <../examples/real_world/franka_emika_panda/reaching_franka_omniverse_isaacgym_env.py>`
                * Training script: :download:`reaching_franka_omniverse_isaacgym_skrl_train.py <../examples/real_world/franka_emika_panda/reaching_franka_omniverse_isaacgym_skrl_train.py>`
                * Evaluation script: :download:`reaching_franka_omniverse_isaacgym_skrl_eval.py <../examples/real_world/franka_emika_panda/reaching_franka_omniverse_isaacgym_skrl_eval.py>`
                * Checkpoints (:literal:`agent_joint.pt`, :literal:`agent_cartesian.pt`): :download:`trained_checkpoints.zip <https://github.com/Toni-SM/skrl/files/9595293/trained_checkpoints.zip>`

                **Training and evaluation:**

                .. code-block:: bash

                    # training (local workstation)
                    ~/.local/share/ov/pkg/isaac_sim-*/python.sh reaching_franka_omniverse_isaacgym_skrl_train.py

                    # training (docker container)
                    /isaac-sim/python.sh reaching_franka_omniverse_isaacgym_skrl_train.py

                .. code-block:: bash

                    # evaluation (local workstation)
                    ~/.local/share/ov/pkg/isaac_sim-*/python.sh reaching_franka_omniverse_isaacgym_skrl_eval.py

                    # evaluation (docker container)
                    /isaac-sim/python.sh reaching_franka_omniverse_isaacgym_skrl_eval.py

                **Main environment configuration:**

                The control space (Cartesian or joint) can be specified in the task configuration dictionary (from :literal:`reaching_franka_omniverse_isaacgym_skrl_train.py`) as follow:

                .. code-block:: python

                    TASK_CFG["task"]["env"]["controlSpace"] = "joint"  # "joint" or "cartesian"

            .. tab:: Simulation (Isaac Gym)

                .. raw:: html

                    <video width="100%" controls autoplay>
                        <source src="https://user-images.githubusercontent.com/22400377/193537523-e0f0f8ad-2295-410c-ba9a-2a16c827a498.mp4" type="video/mp4">
                    </video>

                .. raw:: html

                    <img width="100%" src="https://user-images.githubusercontent.com/22400377/193546966-bcf966e6-98d8-4b41-bc15-bd7364a79381.png">

                |

                **Prerequisites:**

                All installation steps described in Isaac Gym's `Installation <https://github.com/NVIDIA-Omniverse/IsaacGymEnvs#installation>`_ section must be fulfilled

                **Files** (the implementation is self-contained so no specific location is required):

                * Environment: :download:`reaching_franka_isaacgym_env.py <../examples/real_world/franka_emika_panda/reaching_franka_isaacgym_env.py>`
                * Training script: :download:`reaching_franka_isaacgym_skrl_train.py <../examples/real_world/franka_emika_panda/reaching_franka_isaacgym_skrl_train.py>`
                * Evaluation script: :download:`reaching_franka_isaacgym_skrl_eval.py <../examples/real_world/franka_emika_panda/reaching_franka_isaacgym_skrl_eval.py>`

                **Training and evaluation:**

                .. note::

                    The checkpoints obtained in Isaac Gym were not evaluated with the real robot. However, they were evaluated in Omniverse Isaac Gym showing successful performance

                .. code-block:: bash

                    # training (with the Python virtual environment active)
                    python reaching_franka_isaacgym_skrl_train.py

                .. code-block:: bash

                    # evaluation (with the Python virtual environment active)
                    python reaching_franka_isaacgym_skrl_eval.py

                **Main environment configuration:**

                The control space (Cartesian or joint) can be specified in the task configuration dictionary (from :literal:`reaching_franka_isaacgym_skrl_train.py`) as follow:

                .. code-block:: python

                    TASK_CFG["env"]["controlSpace"] = "joint"  # "joint" or "cartesian"

.. _library_utilities:

Library utilities (skrl.utils module)
-------------------------------------

This example shows how to use the library utilities to carry out the post-processing of files and data generated by the experiments

.. tabs::
            
    .. tab:: Tensorboard files
        
        .. image:: ../_static/imgs/utils_tensorboard_file_iterator.svg
            :width: 100%
            :alt: Tensorboard file iterator
        
        .. raw:: html

            <br><br>

        Example of a figure, generated by the code, showing the total reward (left) and the mean and standard deviation (right) of all experiments located in the runs folder
        
        :download:`tensorboard_file_iterator.py <../examples/utils/tensorboard_file_iterator.py>`

        **Note:** The code will load all the Tensorboard files of the experiments located in the :literal:`runs` folder. It is necessary to adjust the iterator's parameters for other paths

        .. literalinclude:: ../examples/utils/tensorboard_file_iterator.py
            :language: python
            :emphasize-lines: 4, 11-13
