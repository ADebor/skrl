Saving, loading and logging
===========================

Tracking metrics (TensorBoard)
------------------------------

Configuration
^^^^^^^^^^^^^

`TensorBoard <https://www.tensorflow.org/tensorboard>`_ is used for tracking and visualizing metrics and scalars (coefficients, losses, etc.). The tracking and writing of metrics and scalars is the responsibility of the agents (**can be customized independently for each agent using its configuration dictionary**)

Each agent offers the following parameters under the :literal:`"experiment"` key

.. code-block:: python
    :emphasize-lines: 5,6,7

    DEFAULT_CONFIG = {
        ...
        
        "experiment": {
            "directory": "",            # experiment's parent directory
            "experiment_name": "",      # experiment name
            "write_interval": 250,      # TensorBoard writing interval (timesteps)

            "checkpoint_interval": 1000,        # interval for checkpoints (timesteps)
            "checkpoint_policy_only": True,     # checkpoint for policy only
        }
    }

* **directory**: directory path where the data generated by the experiments (a subdirectory) are stored. If no value is set, the :literal:`runs` folder (inside the current working directory) will be used (and created if it does not exist)

* **experiment_name**: name of the experiment (subdirectory). If no value is set, it will be the current date and time and the agent's name (e.g. :literal:`22-01-09_22-48-49-816281_DDPG`)

* **write_interval**: interval for writing metrics and values to TensorBoard (default is 250 timesteps). A value equal to or less than 0 disables tracking and writing to TensorBoard

Tracked metrics/scales visualization
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To visualize the tracked metrics/scales, during or after the training, TensorBoard can be launched using the following command in a terminal 

.. code-block:: bash

    tensorboard --logdir=PATH_TO_RUNS_DIRECTORY

.. image:: ../_static/imgs/data_tensorboard.jpg
      :width: 100%
      :align: center
      :alt: TensorBoard panel

|

The following table shows the metrics/scales tracked by each agent ([**+**] all the time, [**-**] only when such a function is enabled in the agent's configuration):

+-----------+--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|Tag        |Metric / Scalar     |.. centered:: DQN|.. centered:: DDPG|.. centered:: TD3|.. centered:: SAC|.. centered:: PPO|
+===========+====================+=================+==================+=================+=================+=================+
|Coefficient|Entropy coefficient |                 |                  |                 |.. centered:: +  |                 |
+-----------+--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|Episode    |Total timesteps     |.. centered:: +  |.. centered:: +   |.. centered:: +  |.. centered:: +  |.. centered:: +  |
+-----------+--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|Exploration|Exploration noise   |                 |.. centered:: +   |.. centered:: +  |                 |                 |
+           +--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|           |Exploration epsilon |.. centered:: +  |                  |                 |                 |                 |
+-----------+--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|Loss       |Policy loss         |                 |.. centered:: +   |.. centered:: +  |.. centered:: +  |.. centered:: +  |
+           +--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|           |Critic loss         |                 |.. centered:: +   |.. centered:: +  |.. centered:: +  |                 |
+           +--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|           |Value loss          |                 |                  |                 |                 |.. centered:: +  |
+           +--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|           |Entropy loss        |                 |                  |                 |.. centered:: -- |.. centered:: -- |
+           +--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|           |Q-network loss      |.. centered:: +  |                  |                 |                 |                 |
+-----------+--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|Policy     |Standard deviation  |                 |                  |                 |                 |.. centered:: +  |
+-----------+--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|Q-network  |Q1                  |                 |.. centered:: +   |.. centered:: +  |.. centered:: +  |                 |
+           +--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|           |Q2                  |                 |                  |.. centered:: +  |.. centered:: +  |                 |
+-----------+--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|Reward     |Instantaneous reward|.. centered:: +  |.. centered:: +   |.. centered:: +  |.. centered:: +  |.. centered:: +  |
+           +--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|           |Total reward        |.. centered:: +  |.. centered:: +   |.. centered:: +  |.. centered:: +  |.. centered:: +  |
+-----------+--------------------+-----------------+------------------+-----------------+-----------------+-----------------+
|Target     |Target              |.. centered:: +  |.. centered:: +   |.. centered:: +  |.. centered:: +  |                 |
+-----------+--------------------+-----------------+------------------+-----------------+-----------------+-----------------+

Tracking custom metrics/scales
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* **Tracking custom data attached to the agent's control and timing logic (recommended)**

  Although the TensorBoard's writing control and timing logic is controlled by the base class Agent, it is possible to track custom data. The :literal:`track_data` method can be used (see :doc:`Agent <../modules/skrl.agents.base_class>` class for more details), passing as arguments the data identification (tag) and the scalar value to be recorded

  For example, to track the current CPU usage, the following code can be used:

  .. code-block:: python

      # assuming agent is an instance of an Agent subclass
      agent.track_data("Resource / CPU usage", psutil.cpu_percent())

* **Tracking custom data directly to Tensorboard**

  It is also feasible to access directly to the `SummaryWriter <https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter>`_ instance through the :literal:`writer` property if it is desired to write directly to Tensorboard, avoiding the base class's control and timing logic

  For example, to write directly to TensorBoard:

  .. code-block:: python

      # assuming agent is an instance of an Agent subclass
      agent.writer.add_scalar("Resource / CPU usage", psutil.cpu_percent(), global_step=1000)

----------------

Model checkpoint
----------------

Saving checkpoints
^^^^^^^^^^^^^^^^^^

The checkpoints are saved in the :literal:`checkpoints` subdirectory of the experiment's directory (its path can be customized using the options described in the previous subsection). The checkpoint name is the current timestep and the key referring to the model (e.g. :literal:`runs/22-01-09_22-48-49-816281_DDPG/checkpoints/2500_policy.pt`)

The checkpoint management, as in the previous case, is the responsibility of the agents (**can be customized independently for each agent using its configuration dictionary**)

.. code-block:: python
    :emphasize-lines: 9,10

    DEFAULT_CONFIG = {
        ...
        
        "experiment": {
            "directory": "",            # experiment's parent directory
            "experiment_name": "",      # experiment name
            "write_interval": 250,      # TensorBoard writing interval (timesteps)

            "checkpoint_interval": 1000,        # interval for checkpoints (timesteps)
            "checkpoint_policy_only": True,     # checkpoint for policy only
        }
    }

* **checkpoint_interval**: interval for checkpoints (default is 1000 timesteps). A value equal to or less than 0 disables the checkpoint creation

* **checkpoint_policy_only**: if set to :literal:`True`, only the policy will be saved (default behaviour), otherwise all the agent's models (policy, value function, critic, .etc) will be checkpointed

Loading checkpoints
^^^^^^^^^^^^^^^^^^^

Checkpoints can be loaded for each of the instantiated models independently via the :literal:`.load(...)` method (`Model.load <../modules/skrl.models.base_class.html#skrl.models.torch.base.Model.load>`_). It accepts the path (relative or absolute) of the checkpoint to load as the only argument

.. note::

    The model instance must have the same architecture/structure as the one used to save the checkpoint. The current implementation load the model's `state_dict <https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict>`_ directly

The following code shows how to load the checkpoint (e.g. :literal:`runs/22-01-09_22-48-49-816281_DDPG/checkpoints/2500_policy.pt`) of an instantiated policy from a specific definition. See the section :ref:`Examples <examples>` for details about how to load control points and use them to evaluate experiments

.. code-block:: python
    :emphasize-lines: 21

    from skrl.models.torch import DeterministicModel

    # Define the model
    class Policy(DeterministicModel):
        def __init__(self, observation_space, action_space, device, clip_actions = False):
            super().__init__(observation_space, action_space, device, clip_actions)

            self.net = nn.Sequential(nn.Linear(self.num_observations, 32),
                                    nn.ReLU(),
                                    nn.Linear(32, 32),
                                    nn.ReLU(),
                                    nn.Linear(32, self.num_actions))

        def compute(self, states, taken_actions):
            return self.net(states)

    # Instantiate the agent's model
    policy = Policy(env.observation_space, env.action_space, device, clip_actions=True)

    # Load the checkpoint
    policy.load("./runs/22-01-09_22-48-49-816281_DDPG/checkpoints/2500_policy.pt")

--------------------

Memory export/import
--------------------

Exporting memories
^^^^^^^^^^^^^^^^^^

Memories can be automatically exported to files at each filling cycle (before data overwriting is performed). Its activation, the output files' format and their path can be modified through the constructor parameters when an instance is created

.. code-block:: python
    :emphasize-lines: 7-9

    from skrl.memories.torch import RandomMemory

    # Instantiate a memory and enable its export
    memory = RandomMemory(memory_size=16, 
                          num_envs=env.num_envs, 
                          device=device, 
                          export=True,
                          export_format="pt",
                          export_directory="./memories")

* **export**: enable or disable the memory export (default is disabled)

* **export_format**: the format of the exported memory (default is :literal:`"pt"`). Supported formats are PyTorch (:literal:`"pt"`), NumPy (:literal:`"np"`) and Comma-separated values (:literal:`"csv"`)

* **export_directory**: the directory where the memory will be exported (default is :literal:`"memory"`)

Importing memories
^^^^^^^^^^^^^^^^^^

TODO :red:`(comming soon)`
